---
title: "Is reproducibility good enough?"
author: "Ivan Stordal"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf:
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography: 
  - reproducibility.bib
  - extra.bib

abstract: The credibility of science depends on whether research findings are robust, reliable, and reproducible. Yet the past two decades have revealed significant challenges across psychology, economics, and biomedical research, where publication bias, small sample sizes, and flexible analytic practices have contributed to what is often called the “replication crisis.” This paper reviews key insights from the lecture Robust and Reliable Science alongside recent literature, distinguishing between reproducibility and replicability, and examining how large-scale replication efforts and methodological critiques have exposed weaknesses in current practices.

The discussion evaluates whether reproducibility is “good enough” today, arguing that while computational reproducibility should be the baseline for credibility, full replication is more demanding and often resource-intensive. Technical frameworks such as Quarto and GitHub, combined with stronger journal policies, preregistration, and training in open science practices, offer pathways toward more reliable research. The conclusion highlights that progress is real but incomplete: reproducibility is not yet sufficient, but enforceable standards and practical tools are steadily moving science in the right direction.
---

## Introduction

Scientific credibility depends on whether findings are robust and reliable. In recent years, concerns over the so-called “replication crisis” have emerged across disciplines, raising doubts about the reproducibility of published results [@ioannidis2005b]. While reproducibility refers to the ability to re-obtain results with the same data and code, replicability involves validating results with new data [@goodman2016b]. Both are essential for cumulative science, yet evidence suggests that current practices are not always sufficient.

This paper examines whether research is reproducible “enough” today. Drawing on insights from the lecture Robust and Reliable Science and relevant literature, it reviews core problems such as publication bias and Type I errors, large replication studies, and proposed solutions. It then discusses whether replicability should be treated as a norm, how new tools such as Quarto may help, and what barriers remain.


## Literature review

The distinction between reproducibility and replicability is central in the debate on robust and reliable science. Reproducibility typically refers to obtaining the same results using the original data and analysis pipeline, while replicability involves confirming findings with new data [@peng2011; @goodman2016b]. Scholars argue that reproducibility is the minimum standard for credibility, whereas replicability represents the gold standard [@munafo2017].

A major barrier to reliability is publication bias, often described as the “file drawer problem” [@rosenthal1979]. Positive and novel results are more likely to be published, creating a distorted scientific record. This is compounded by the high risk of Type I errors, particularly when researchers have flexibility in data collection and analysis [@simmons2011]. Meta-analyses show that these biases inflate reported effect sizes, reducing the reliability of the literature [@button2013].


Large-scale replication projects highlight the scope of the problem. The Open Science Collaboration (2015) attempted to replicate 100 studies in psychology and succeeded in reproducing only about one third of the effects. Similarly, in economics, @dewald1986b showed that many published empirical results could not be replicated due to missing data or code. Historical initiatives such as the JCMB Project (1982) were early attempts to establish standards for data archiving.

One widely promoted solution has been the requirement that journals mandate submission of code and data alongside articles [@stodden2010; @stodden2013]. This has gradually been extended to include permanent archives and data repositories. Complementary initiatives like preregistration and registered reports aim to reduce questionable research practices [@nosek2015]. Technical frameworks such as literate programming [@gentleman2005b; @xie2015], reproducible computational pipelines [@sandve2013], and modern publishing platforms like Quarto [@allaire2020b] provide tools for integrating data, code, and narrative into a transparent workflow.


## Discussion of the reseach question

-   Should replicability be the norm or is this to much to ask for now?
-   Can Quarto documents help with reproducibility?
-   What problems remains and how can these be solved?

Large-scale replication projects highlight the scope of the problem. The Open Science Collaboration (2015) attempted to replicate 100 studies in psychology and succeeded in reproducing only about one third of the effects. Similarly, in economics, @dewald1986b showed that many published empirical results could not be replicated due to missing data or code. Historical initiatives such as the JCMB Project in 1982 were early attempts to establish standards for data archiving.

One widely promoted solution has been the requirement that journals mandate submission of code and data alongside articles [@stodden2010; @stodden2013]. This has gradually been extended to include permanent archives and data repositories. Complementary initiatives like preregistration and registered reports aim to reduce questionable research practices [@nosek2015]. Technical frameworks such as literate programming [@gentleman2005b; @xie2015], reproducible computational pipelines [@sandve2013], and modern publishing platforms like Quarto [@allaire2020b] provide tools for integrating data, code, and narrative into a transparent workflow.


## Discussion of the reseach question

Should replicability be the norm, or is it too much to ask for now?
There is broad agreement that computational reproducibility should be treated as a baseline expectation for all published research [@peng2011]. Full replication, however, remains resource-intensive and may not be feasible across all disciplines. Scholars suggest prioritizing replication in high-stakes areas, while enforcing reproducibility everywhere [@munafo2017].

Can Quarto documents help with reproducibility?
Tools such as Quarto offer a promising pathway toward more robust science. By embedding code, text, and results in a single document, Quarto reduces the opportunity for errors caused by manual copy–paste workflows [@xie2020; @wickham2016]. Integration with GitHub further improves transparency and enables collaborative review. Educational studies suggest that adopting such tools lowers barriers to reproducibility and helps establish open science practices early in training [@broman2018].

What problems remain and how can these be solved?
Despite technical progress, cultural and institutional barriers persist. Incentives still favor novel, positive results over careful replication [@begley2012]. Underpowered studies and flexible analysis pipelines contribute to irreproducible findings [@button2013]. Even where data-sharing policies exist, enforcement is inconsistent [@miguel2014].

Several solutions have been proposed:

- Policy and enforcement – stronger journal requirements for code and data, with checks   at the review stage [@stodden2013].

- Preregistration and registered reports – reducing publication bias by committing to     hypotheses and analyses before seeing results [@nosek2015].

- Training and infrastructure – equipping researchers with open science tools,
  reproducible pipelines, and collaborative platforms [@sandve2013].

- Environment capture – using containers or dependency management (e.g., renv in R) to    ensure analyses remain reproducible over time [@starr2015].


## Conclusion

The debate over reproducibility and replicability highlights both encouraging progress and persistent challenges. Evidence from psychology, economics, and biomedicine demonstrates that many findings cannot be easily reproduced [@opensciencecollaboration2015; @dewald1986b; @begley2012]. At the same time, reforms such as preregistration, journal data policies, and computational tools have made reproducible workflows increasingly achievable [@sandve2013; @munafo2017].

The central conclusion is that reproducibility is not yet good enough. Technical barriers have been lowered, but cultural incentives and enforcement gaps continue to hinder progress. A pragmatic solution is to treat computational reproducibility as a baseline requirement, while prioritizing replication in high-impact areas. With the continued adoption of tools like Quarto, GitHub, and transparent data archives, the scientific community can move toward truly robust and reliable science.


## References

Barnes, N. (2010). Publish your computer code: it is good enough. Nature, 467(7317), 753.

Bechhofer, S., De Roure, D., Gamble, M., Goble, C., & Buchan, I. (2013). Research objects: Towards exchange and reuse of digital knowledge. The Future of Scholarly Communication.

Broman, K. W., & Woo, K. H. (2018). Data organization in spreadsheets. The American Statistician, 72(1), 2–10.

Dewald, W. G., Thursby, J. G., & Anderson, R. G. (1986). Replication in empirical economics: The Journal of Money, Credit and Banking Project. The American Economic Review, 76(4), 587–603.

Gentleman, R., & Temple Lang, D. (2005). Statistical analyses and reproducible research. Bioconductor Project Working Papers, 2.

Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12.

Nosek, B. A., et al. (2015). Promoting an open research culture. Science, 348(6242), 1422–1425.

Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251).

Peng, R. D. (2011). Reproducible research in computational science. Science, 334(6060), 1226–1227.

Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3), 638–641.

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366.

Stodden, V. (2010). The scientific method in practice: Reproducibility in the computational sciences. MIT Sloan Research Paper.

Stodden, V., Leisch, F., & Peng, R. D. (2013). Implementing reproducible research. CRC Press.

Xie, Y. (2015). Dynamic documents with R and knitr. Chapman and Hall/CRC.

Xie, Y. (2020). R Markdown: The definitive guide. CRC Press.

Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Sert, N. P., Simonsohn, U., Wagenmakers, E.-J., Ware, J. J., & Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(1), 0021. https://doi.org/10.1038/s41562-016-0021

Sandve, G. K., Nekrutenko, A., Taylor, J., & Hovig, E. (2013). Ten simple rules for reproducible computational research. PLoS Computational Biology, 9(10), e1003285. https://doi.org/10.1371/journal.pcbi.1003285

Begley, C. G., & Ellis, L. M. (2012). Raise standards for preclinical cancer research. Nature, 483(7391), 531–533. https://doi.org/10.1038/483531a

Miguel, E., Camerer, C., Casey, K., Cohen, J., Esterling, K. M., Gerber, A., Glennerster, R., Green, D. P., Humphreys, M., Imbens, G., Laitin, D., Madon, T., Nelson, L. D., Nosek, B. A., Petersen, M., Sedlmayr, R., Simmons, J. P., & Simonsohn, U. (2014). Promoting transparency in social science research. Science, 343(6166), 30–31. https://doi.org/10.1126/science.1245317

Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376. https://doi.org/10.1038/nrn3475






