---
title: "Is reproducibility good enough?"
author: "Ivan Stordal"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf:
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography: 
  - reproducibility.bib
  - extra.bib

abstract: "A very short abstract. Put the abstract text here. One or two paragraphs summarising what follows below."
---

## Introduction

What is this paper about?
What is discussed?
Why is it of any consequence?

## Literature review

The distinction between reproducibility and replicability is central in the debate on robust and reliable science. Reproducibility typically refers to obtaining the same results using the original data and analysis pipeline, while replicability involves confirming findings with new data [@peng2011; @goodman2016b]. Scholars argue that reproducibility is the minimum standard for credibility, whereas replicability represents the gold standard [@munafo2017].

A major barrier to reliability is publication bias, often described as the “file drawer problem” [@rosenthal1979]. Positive and novel results are more likely to be published, creating a distorted scientific record. This is compounded by the high risk of Type I errors, particularly when researchers have flexibility in data collection and analysis [@simmons2011]. Meta-analyses show that these biases inflate reported effect sizes, reducing the reliability of the literature [@button2013].

Large-scale replication projects highlight the scope of the problem. The Open Science Collaboration (2015) attempted to replicate 100 studies in psychology and succeeded in reproducing only about one third of the effects. Similarly, in economics, @dewald1986b showed that many published empirical results could not be replicated due to missing data or code. Historical initiatives such as the JCMB Project (1982) were early attempts to establish standards for data archiving.

One widely promoted solution has been the requirement that journals mandate submission of code and data alongside articles [@stodden2010; @stodden2013]. This has gradually been extended to include permanent archives and data repositories. Complementary initiatives like preregistration and registered reports aim to reduce questionable research practices [@nosek2015]. Technical frameworks such as literate programming [@gentleman2005b; @xie2015], reproducible computational pipelines [@sandve2013], and modern publishing platforms like Quarto [@allaire2020b] provide tools for integrating data, code, and narrative into a transparent workflow.

Smart stuff from others about the topic.

Use a least 20 citations, a least 5 of them must be new (not from the provided .bib file).

Use both in-line and normal citations.

Example:

@gentleman2005 argues that bla bla bla.
On the other hand it's claimed that bla bla [@barbalorenaa.2018; @bartlett2008].

## Discussion of the reseach question

-   Should replicability be the norm or is this to much to ask for now?
-   Can Quarto documents help with reproducibility?
-   What problems remains and how can these be solved?

## Conclusion

## References

and

-   Version number and reference to packages used
-   R version used




